{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Sure! Here's an explanation of the parameters you mentioned:\n",
      "\n",
      "1. **Temperature**: This parameter controls the randomness of the generated text. A higher temperature value results in more randomness and creativity in the generated responses, while a lower temperature value produces more conservative and predictable outputs.\n",
      "\n",
      "2. **Max_tokens**: This parameter specifies the maximum number of tokens (words or subwords) in the generated response. It determines the length of the generated text.\n",
      "\n",
      "3. **Top_p**: Also known as nucleus sampling, this parameter controls the diversity of the generated responses by setting a threshold for the cumulative probability of the most likely tokens to be included in the output. It helps in generating more diverse and varied responses.\n",
      "\n",
      "4. **Frequency_penalty**: This parameter penalizes the model from generating the same tokens repeatedly in the output. It helps in increasing the diversity of the generated responses by encouraging the model to explore different words.\n",
      "\n",
      "5. **Presence_penalty**: This parameter encourages the model to cover a wider range of topics in the generated responses by penalizing the repetition of similar themes or topics. It helps in promoting variety and coherence in the generated text.\n",
      "\n",
      "6. **N**: This parameter specifies the number of completions to generate for a prompt. It allows you to generate multiple responses for a single input prompt.\n",
      "\n",
      "7. **Stop**: This parameter allows you to specify a sequence of tokens at which the model should stop generating text. It helps in controlling the length and structure of the generated responses by providing a stopping point for the\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"Secret\n",
    "\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"you are a chatgpt expert in the chatgpt api\"},\n",
    "        {\"role\": \"user\", \"content\": \"**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`. can you explain it ?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 300,\n",
    "    \"top_p\": 1.0,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"presence_penalty\": 0.0,\n",
    "    \"n\": 1,\n",
    "    \"stop\": None\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    assistant_reply = response_data['choices'][0]['message']['content']\n",
    "    print(\"Assistant:\", assistant_reply)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(long_text_input, temperature=0.5, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, logprobs=None):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    api_key = \"Secret\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarization expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following text: {long_text_input}\"}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"logprobs\": logprobs\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        summary = response_data['choices'][0]['message']['content']\n",
    "        return summary\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = '''I procrastinated a deep dive into transformers for a few years. Finally the discomfort of not knowing what makes them tick grew too great for me. Here is that dive.\n",
    "\n",
    "Transformers were introduced in this 2017 paper as a tool for sequence transduction—converting one sequence of symbols to another. The most popular examples of this are translation, as in English to German. It has also been modified to perform sequence completion—given a starting prompt, carry on in the same vein and style. They have quickly become an indispensible tool for research and product development in natural language processing.\n",
    "\n",
    "Before we start, just a heads-up. We're going to be talking a lot about matrix multiplications and touching on backpropagation (the algorithm for training the model), but you don't need to know any of it beforehand. We'll add the concepts we need one at a time, with explanation.\n",
    "\n",
    "This isn't a short journey, but I hope you'll be glad you came.\n",
    "\n",
    "► One-hot encoding\n",
    "► Dot product\n",
    "► Matrix multiplication\n",
    "► Matrix multiplication as a table lookup\n",
    "► First order sequence model\n",
    "► Second order sequence model\n",
    "► Second order sequence model with skips\n",
    "► Masking\n",
    "► Rest Stop and an Off Ramp\n",
    "► Attention as matrix multiplication\n",
    "► Second order sequence model as matrix multiplications\n",
    "► Sequence completion\n",
    "► Embeddings\n",
    "► Positional encoding\n",
    "► De-embeddings\n",
    "► Softmax\n",
    "► Multi-head attention\n",
    "► Single head attention revisited\n",
    "► Skip connection\n",
    "► Multiple layers\n",
    "► Decoder stack\n",
    "► Encoder stack\n",
    "► Cross-attention\n",
    "► Tokenizing\n",
    "► Byte pair encoding\n",
    "► Audio input\n",
    "► Resources and credits\n",
    "One-hot encoding\n",
    "In the beginning were the words. So very many words. Our first step is to convert all the words to numbers so we can do math on them.\n",
    "\n",
    "Imagine that our goal is to create the computer that responds to our voice commands. It’s our job to build the transformer that converts (or transduces) a sequence of sounds to a sequence of words.\n",
    "\n",
    "We start by choosing our vocabulary, the collection of symbols that we are going to be working with in each sequence. In our case, there will be two different sets of symbols, one for the input sequence to represent vocal sounds and one for the output sequence to represent words.\n",
    "\n",
    "For now, let's assume we're working with English. There are tens of thousands of words in the English language, and perhaps another few thousand to cover computer-specific terminology. That would give us a vocabulary size that is the better part of a hundred thousand. One way to convert words to numbers is to start counting at one and assign each word its own number. Then a sequence of words can be represented as a list of numbers.\n",
    "\n",
    "For example, consider a tiny language with a vocabulary size of three: files, find, and my. Each word could be swapped out for a number, perhaps files = 1, find = 2, and my = 3. Then the sentence \"Find my files\", consisting of the word sequence [ find, my, files ] could be represented instead as the sequence of numbers [2, 3, 1].\n",
    "\n",
    "This is a perfectly valid way to convert symbols to numbers, but it turns out that there's another format that's even easier for computers to work with, one-hot encoding. In one-hot encoding a symbol is represented by an array of mostly zeros, the same length of the vocabulary, with only a single element having a value of one. Each element in the array corresponds to a separate symbol.\n",
    "\n",
    "Another way to think about one-hot encoding is that each word still gets assigned its own number, but now that number is an index to an array. Here is our example above, in one-hot notation.\n",
    "\n",
    "A one-hot encoded vocabulary\n",
    "\n",
    "So the sentence \"Find my files\" becomes a sequence of one-dimensional arrays, which, after you squeeze them together, starts to look like a two-dimensional array.\n",
    "\n",
    "A one-hot encoded sentence\n",
    "\n",
    "Heads-up, I'll be using the terms \"one-dimensional array\" and \"vector\" interchangeably. Likewise with \"two-dimensional array\" and \"matrix\".\n",
    "\n",
    "Dot product\n",
    "One really useful thing about the one-hot representation is that it lets us compute dot products. These are also known by other intimidating names like inner product and scalar product. To get the dot product of two vectors, multiply their corresponding elements, then add the results.\n",
    "\n",
    "Dot product illustration\n",
    "\n",
    "Dot products are especially useful when we're working with our one-hot word representations. The dot product of any one-hot vector with itself is one.\n",
    "\n",
    "Dot product of matching vectors\n",
    "\n",
    "And the dot product of any one-hot vector with any other one-hot vector is zero.\n",
    "\n",
    "Dot product of non-matching vectors\n",
    "\n",
    "The previous two examples show how dot products can be used to measure similarity. As another example, consider a vector of values that represents a combination of words with varying weights. A one-hot encoded word can be compared against it with the dot product to show how strongly that word is represented.\n",
    "\n",
    "Dot product gives the similarity between two vectors\n",
    "\n",
    "Matrix multiplication\n",
    "The dot product is the building block of matrix multiplication, a very particular way to combine a pair of two-dimensional arrays. We'll call the first of these matrices A and the second one B. In the simplest case, when A has only one row and B has only one column, the result of matrix multiplication is the dot product of the two.\n",
    "\n",
    "multiplication of a single row matrix and a single column matrix\n",
    "\n",
    "Notice how the number of columns in A and the number of rows in B needs to be the same for the two arrays to match up and for the dot product to work out.\n",
    "\n",
    "When A and B start to grow, matrix multiplication starts to get trippy. To handle more than one row in A, take the dot product of B with each row separately. The answer will have as many rows as A does.\n",
    "\n",
    "multiplication of a two row matrix and a single column matrix\n",
    "\n",
    "When B takes on more columns, take the dot product of each column with A and stack the results in successive columns.\n",
    "\n",
    "multiplication of a one row matrix and a two column matrix\n",
    "\n",
    "Now we can extend this to mutliplying any two matrices, as long as the number of columns in A is the same as the number of rows in B. The result will have the same number of rows as A and the same number of columns as B.\n",
    "\n",
    "multiplication of a one three matrix and a two column matrix\n",
    "\n",
    "If this is the first time you're seeing this, it might feel needlessly complex, but I promise it pays off later.\n",
    "\n",
    "Matrix multiplication as a table lookup\n",
    "Notice how matrix multiplication acts as a lookup table here. Our A matrix is made up of a stack of one-hot vectors. They have ones in the first column, the fourth column, and the third column, respectively. When we work through the matrix multiplication, this serves to pull out the first row, the fourth row, and the third row of the B matrix, in that order. This trick of using a one-hot vector to pull out a particular row of a matrix is at the core of how transformers work.\n",
    "\n",
    "First order sequence model\n",
    "We can set aside matrices for a minute and get back to what we really care about, sequences of words. Imagine that as we start to develop our natural language computer interface we want to handle just three possible commands:\n",
    "\n",
    "Show me my directories please.\n",
    "Show me my files please.\n",
    "Show me my photos please.\n",
    "Our vocabulary size is now seven:\n",
    "{directories, files, me, my, photos, please, show}.\n",
    "One useful way to represent sequences is with a transition model. For every word in the vocabulary, it shows what the next word is likely to be. If users ask about photos half the time, files 30% of the time, and directories the rest of the time, the transition model will look like this. The sum of the transitions away from any word will always add up to one.\n",
    "\n",
    "Markov chain transition model\n",
    "\n",
    "This particular transition model is called a Markov chain, because it satisfies the Markov property that the probabilities for the next word depend only on recent words. More specifically, it is a first order Markov model because it only looks at the single most recent word. If it considered the two most recent words it would be a second order Markov model.\n",
    "\n",
    "Our break from matrices is over. It turns out that Markov chains can be expressed conveniently in matrix form. Using the same indexing scheme that we used when creating one-hot vectors, each row represents one of the words in our vocabulary. So does each column. The matrix transition model treats a matrix as a lookup table. Find the row that corresponds to the word you’re interested in. The value in each column shows the probability of that word coming next. Because the value of each element in the matrix represents a probability, they will all fall between zero and one. Because probabilities always sum to one, the values in each row will always add up to one.\n",
    "\n",
    "Transition matrix\n",
    "\n",
    "In the transition matrix here we can see the structure of our three sentences clearly. Almost all of the transition probabilities are zero or one. There is only one place in the Markov chain where branching happens. After my, the words directories, files, or photos might appear, each with a different probability. Other than that, there’s no uncertainty about which word will come next. That certainty is reflected by having mostly ones and zeros in the transition matrix.\n",
    "\n",
    "We can revisit our trick of using matrix multiplication with a one-hot vector to pull out the transition probabilities associated with any given word. For instance, if we just wanted to isolate the probabilities of which word comes after my, we can create a one-hot vector representing the word my and multiply it by our transition matrix. This pulls out the relevant row and shows us the probability distribution of what the next word will be.\n",
    "\n",
    "Transition probability lookup\n",
    "\n",
    "Second order sequence model\n",
    "Predicting the next word based on only the current word is hard. That's like predicting the rest of a tune after being given just the first note. Our chances are a lot better if we can at least get two notes to go on.\n",
    "\n",
    "We can see how this works in another toy language model for our computer commands. We expect that this one will only ever see two sentences, in a 40/60 proportion.\n",
    "\n",
    "Check whether the battery ran down please.\n",
    "Check whether the program ran please.\n",
    "A Markov chain illustrates a first order model for this.\n",
    "Another first order Markov chain transition model\n",
    "\n",
    "Here we can see that if our model looked at the two most recent words, instead of just one, that it could do a better job. When it encounters battery ran, it knows that the next word will be down, and when it sees program ran the next word will be please. This eliminates one of the branches in the model, reducing uncertainty and increasing confidence. Looking back two words turns this into a second order Markov model. It gives more context on which to base next word predictions. Second order Markov chains are more challenging to draw, but here are the connections that demonstrate their value.\n",
    "\n",
    "Second order Markov chain\n",
    "\n",
    "To highlight the difference between the two, here is the first order transition matrix,\n",
    "\n",
    "Another first order transition matrix\n",
    "\n",
    "and here is the second order transition matrix.\n",
    "\n",
    "Second order transition matrix\n",
    "\n",
    "Notice how the second order matrix has a separate row for every combination of words (most of which are not shown here). That means that if we start with a vocabulary size of N then the transition matrix has N^2 rows.\n",
    "\n",
    "What this buys us is more confidence. There are more ones and fewer fractions in the second order model. There's only one row with fractions in it, one branch in our model. Intuitively, looking at two words instead of just one gives more context, more information on which to base a next word guess.\n",
    "\n",
    "Second order sequence model with skips\n",
    "A second order model works well when we only have to look back two words to decide what word comes next. What about when we have to look back further? Imagine we are building yet another language model. This one only has to represent two sentences, each equally likely to occur.\n",
    "\n",
    "Check the program log and find out whether it ran please.\n",
    "Check the battery log and find out whether it ran down please.\n",
    "In this example, in order to determine which word should come after ran, we would have to look back 8 words into the past. If we want to improve on our second order language model, we can of course consider third- and higher order models. However, with a significant vocabulary size this takes a combination of creativity and brute force to execute. A naive implementation of an eighth order model would have N^8 rows, a ridiculous number for any reasonable vocubulary.\n",
    "\n",
    "Instead, we can do something sly and make a second order model, but consider the combinations of the most recent word with each of the words that came before. It's still second order, because we're only considering two words at a time, but it allows us to reach back further and capture long range dependencies. The difference between this second-order-with-skips and a full umpteenth-order model is that we discard most of the word order information and combinations of preceeeding words. What remains is still pretty powerful.\n",
    "\n",
    "Markov chains fail us entirely now, but we can still represent the link between each pair of preceding words and the words that follow. Here we've dispensed with numerical weights, and instead are showing only the arrows associated with non-zero weights. Larger weights are shown with heavier lines.\n",
    "\n",
    "Second order with skips feature voting\n",
    "\n",
    "Here's what it might look like in a transition matrix.\n",
    "\n",
    "Second order with skips transition matrix\n",
    "\n",
    "This view only shows the rows relevant to predicting the word that comes after ran. It shows instances where the most recent word (ran) is preceded by each of the other words in the vocabulary. Only the relevant values are shown. All the empty cells are zeros.\n",
    "\n",
    "The first thing that becomes apparent is that, when trying to predict the word that comes after ran, we no longer look at just one line, but rather a whole set of them. We've moved out of the Markov realm now. Each row no longer represents the state of the sequence at a particular point. Instead, each row represents one of many features that may describe the sequence at a particular point. The combination of the most recent word with each of the words that came before makes for a collection of applicable rows, maybe a large collection. Because of this change in meaning, each value in the matrix no longer represents a probability, but rather a vote. Votes will be summed and compared to determine next word predictions.\n",
    "\n",
    "The next thing that becomes apparent is that most of the features don't matter. Most of the words appear in both sentences, and so the fact that they have been seen is of no help in predicting what comes next. They all have a value of .5. The only two exceptions are battery and program. They have some 1 and 0 weights associated with the two cases we're trying to distinguish. The feature battery, ran indicates that ran was the most recent word and that battery occurred somewhere earlier in the sentence. This feature has a weight of 1 associated with down and a weight of 0 associated with please. Similarly, the feature program, ran has the opposite set of weights. This structure shows that it is the presence of these two words earlier in the sentence that is decisive in predicting which word comes next.\n",
    "\n",
    "To convert this set of word-pair features into a next word estimate, the values of all the relevant rows need to be summed. Adding down the column, the sequence Check the program log and find out whether it ran generates sums of 0 for all the words, except a 4 for down and a 5 for please. The sequence Check the battery log and find out whether it ran does the same, except with a 5 for down and a 4 for please. By choosing the word with the highest vote total as the next word prediction, this model gets us the right answer, despite having an eight word deep dependency.\n",
    "\n",
    "Masking\n",
    "On more careful consideration, this is unsatisfying. The difference between a vote total of 4 and 5 is relatively small. It suggests that the model isn't as confident as it could be. And in a larger, more organic language model it's easy to imagine that such a slight difference could be lost in the statistical noise.\n",
    "\n",
    "We can sharpen the prediction by weeding out all the uninformative feature votes. With the exception of battery, ran and program, ran. It's helpful to remember at this point that we pull the relevant rows out of the transition matrix by multiplying it with a vector showing which features are currently active. For this example so far, we've been using the implied feature vector shown here.\n",
    "\n",
    "Feature selection vector\n",
    "\n",
    "It includes a one for each feature that is a combination of ran with each of the words that come before it. Any words that come after it don't get included in the feature set. (In the next word prediction problem these haven't been seen yet, and so it's not fair to use them predict what comes next.) And this doesn't include all the other possible word combinations. We can safely ignore these for this example because they will all be zero.\n",
    "\n",
    "To improve our results, we can additionally force the unhelpful features to zero by creating a mask. It's a vector full of ones except for the positions you'd like to hide or mask, and those are set to zero. In our case we'd like to mask everything except for battery, ran and program, ran, the only two features that have been of any help.\n",
    "\n",
    "Masked feature activities\n",
    "\n",
    "To apply the mask, we multiply the two vectors element by element. Any feature activity value in an unmasked position will be multiplied by one and left unchanged. Any feature activity value in a masked position will be multiplied by zero, and thus forced to zero.\n",
    "\n",
    "The mask has the effect of hiding a lot of the transition matrix. It hides the combination of ran with everything except battery and program, leaving just the features that matter.\n",
    "\n",
    "Masked transition matrix\n",
    "\n",
    "After masking the unhelpful features, the next word predictions become much stronger. When the word battery occurs earlier in the sentence, the word after ran is predicted to be down with a weight of 1 and please with a weight of 0. What was a weight difference of 25 percent has become a difference of infinity percent. There is no doubt what word comes next. The same strong prediction occurs for please when program occurs early on.\n",
    "\n",
    "This process of selective masking is the attention called out in the title of the original paper on transformers. So far, what we've descibed is a just an approximation of how attention is implemented in the paper. It captures the important concepts, but the details are different. We'll close that gap later.\n",
    "\n",
    "Rest Stop and an Off Ramp\n",
    "Congratulations on making it this far. You can stop if you want. The selective-second-order-with-skips model is a useful way to think about what transformers do, at least in the decoder side. It captures, to a first approximation, what generative language models like OpenAI's GPT-3 are doing. It doesn't tell the complete story, but it represents the central thrust of it.\n",
    "\n",
    "The next sections cover more of the gap between this intuitive explanation and how transformers are implemented. These are largely driven by three practical considerations.\n",
    "\n",
    "Computers are especially good at matrix multiplications. There is an entire industry around building computer hardware specifically for fast matrix multiplications. Any computation that can be expressed as a matrix multiplication can be made shockingly efficient. It's a bullet train. If you can get your baggage into it, it will get you where you want to go real fast.\n",
    "Each step needs to be differentiable. So far we've just been working with toy examples, and have had the luxury of hand-picking all the transition probabilities and mask values—the model parameters. In practice, these have to be learned via backpropagation, which depends on each computation step being differentiable. This means that for any small change in a parameter, we can calculate the corresponding change in the model error or loss.\n",
    "The gradient needs to be smooth and well conditioned. The combination of all the derivatives for all the parameters is the loss gradient. In practice, getting backpropagation to behave well requires gradients that are smooth, that is, the slope doesn’t change very quickly as you make small steps in any direction. They also behave much better when the gradient is well conditioned, that is, it’s not radically larger in one direction than another. If you picture a loss function as a landscape, The Grand Canyon would be a poorly conditioned one. Depending on whether you are traveling along the bottom, or up the side, you will have very different slopes to travel. By contrast, the rolling hills of the classic Windows screensaver would have a well conditioned gradient.\n",
    "If the science of architecting neural networks is creating differentiable building blocks, the art of them is stacking the pieces in such a way that the gradient doesn’t change too quickly and is roughly of the same magnitude in every direction.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The text provides an in-depth exploration of transformers, starting with an introduction to their use in sequence transduction and their importance in natural language processing. It delves into concepts such as one-hot encoding, dot product, matrix multiplication, first and second-order sequence models, masking, attention mechanisms, and the practical considerations involved in implementing transformers. The discussion also covers the significance of matrix multiplication for efficient computation, the need for differentiability in each step, and the importance of smooth and well-conditioned gradients in backpropagation. The text concludes with insights into the design and architecture of neural networks, emphasizing the balance required for effective model training and optimization.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "summary = summarize_text(inputText)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text_to_translate, source_language, target_language, temperature=0.5, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, logit_bias=None):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    api_key = \"Secret\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a translation expert. Translate from {source_language} to {target_language}.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Translate the following text: {text_to_translate}\"}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"logit_bias\": logit_bias\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        translation = response_data['choices'][0]['message']['content']\n",
    "        return translation\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: اسمي ماجد، أنا طالب في جامعة الملك سعود ولدي درجة في الهندسة الكهربائية\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"my name is majed, i am a student at the university of king saud university and i have a degree in electrical engineering\"\n",
    "source_language = \"English\"\n",
    "target_language = \"Arabic\"\n",
    "translation = translate_text(text_to_translate, source_language, target_language)\n",
    "print(\"Translation:\", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_sentiment(api_key, text, temperature=0.5, max_tokens=60, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, n=1, logprobs=None):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    api_key=\"Secret\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Determine the sentiment of the following text: {text}\"}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"n\": n,\n",
    "        \"logprobs\": logprobs\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        sentiment = response_data['choices'][0]['message']['content']\n",
    "        return sentiment.strip()\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: The sentiment of the text is neutral. It simply provides factual information about the person being a student at the University of King Saud University and having a degree in electrical engineering.\n"
     ]
    }
   ],
   "source": [
    "text = \"i am majed, i am a student at the university of king saud university and i have a degree in electrical engineering\"\n",
    "sentiment = analyze_sentiment(api_key, text)\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: Once upon a time in a land far away, there was a princess who lived in a magnificent castle surrounded by lush gardens and sparkling lakes.\n"
     ]
    }
   ],
   "source": [
    "def complete_text(api_key, prompt, temperature=0.7, max_tokens=150, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=None):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a text completion expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Complete the following text: {prompt}\"}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"stop\": stop\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        completion = response_data['choices'][0]['message']['content']\n",
    "        return completion.strip()\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Example usage:\n",
    "api_key = \"Secret\n",
    "prompt = \"Once upon a time in a land far away, there was a\"\n",
    "completion = complete_text(api_key, prompt)\n",
    "print(\"Completion:\", completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FTF2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
